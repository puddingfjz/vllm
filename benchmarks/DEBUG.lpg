Namespace(backend='vllm', dataset='ShareGPT_V3_unfiltered_cleaned_split.json', input_len=None, output_len=None, model='huggyllama/llama-7b', tokenizer='huggyllama/llama-7b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=100, seed=0, hf_max_batch_size=None, trust_remote_code=False, max_model_len=None, dtype='auto')
INFO 12-17 02:39:25 llm_engine.py:73] Initializing an LLM engine with config: model='huggyllama/llama-7b', tokenizer='huggyllama/llama-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 12-17 02:39:25 tokenizer.py:32] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.
NCCL version 2.18.1+cuda12.1
Run Init workers
world size: 1, rank: 0, init_method: tcp://localhost:42061
world_size: 1, tensor_model_parallel_size: 1, pipeline_model_parallel_size: 1
num_tensor_model_parallel_groups: 1, num_pipeline_model_parallel_groups: 1
rank: 0
_TENSOR_MODEL_PARALLEL_GROUP ranks: range(0, 1)
_PIPELINE_MODEL_PARALLEL_GROUP ranks: range(0, 1)
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
tp_size in LlamaAttention: 1
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): VocabParallelEmbedding()
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (qkv_proj): QKVParallelLinear()
          (o_proj): RowParallelLinear()
          (rotary_emb): RotaryEmbedding()
          (attn): PagedAttention()
        )
        (mlp): LlamaMLP(
          (gate_up_proj): MergedColumnParallelLinear()
          (down_proj): RowParallelLinear()
          (act_fn): SiluAndMul()
        )
        (input_layernorm): RMSNorm()
        (post_attention_layernorm): RMSNorm()
      )
    )
    (norm): RMSNorm()
  )
  (lm_head): ParallelLMHead()
  (sampler): Sampler()
)
init model size: 12.551765441894531 GB
visited_param_names[30]: ['model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.qkv_proj.weight']
weight_range_in_blk: [(0, 4096, torch.Size([4096])), (4096, 45088768, torch.Size([4096, 11008])), (45092864, 90177536, torch.Size([22016, 4096])), (135270400, 4096, torch.Size([4096])), (135274496, 16777216, torch.Size([4096, 4096])), (152051712, 50331648, torch.Size([12288, 4096]))]
weight_num_per_layer: 202383360
release param tensors: 0.48877716064453125 GB
after allocate param cache: 12.174797058105469 GB
loading: 13.485862617962994 4.552502650767565e-05 cuda:0
param size: 13476831232
INFO 12-17 02:39:43 llm_engine.py:224] # GPU blocks: 7499, # CPU blocks: 512
step_i: 300
Throughput: 0.98 requests/s, 470.29 tokens/s
Generated:
