Namespace(backend='vllm', dataset='ShareGPT_V3_unfiltered_cleaned_split.json', model='huggyllama/llama-7b', tokenizer='huggyllama/llama-7b', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=1000, seed=0, hf_max_batch_size=None, trust_remote_code=False, dtype='auto')
INFO 11-08 01:40:56 tokenizer.py:31] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.
len(filtered_dataset): 9
len(sampled_requests): 9
len(sampled_requests): 1008
INFO 11-08 01:41:15 llm_engine.py:76] Initializing an LLM engine with config: model='huggyllama/llama-7b', tokenizer='huggyllama/llama-7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)
INFO 11-08 01:41:15 tokenizer.py:31] For some LLaMA V1 models, initializing the fast tokenizer may take a long time. To reduce the initialization time, consider using 'hf-internal-testing/llama-tokenizer' instead of the original tokenizer.
INFO 11-08 01:41:20 llm_engine.py:218] # GPU blocks: 1372, # CPU blocks: 7424
self.use_our_method: False
step_i: 250, step_start: 250, step_end:350
step_i: 350, step_start: 250, step_end:350
Throughput: 5.42 requests/s, 6117.74 tokens/s
